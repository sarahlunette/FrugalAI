{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd89c7-05a6-4a9a-85cf-cbb79695865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that could be useful\n",
    "# How to load audio from path\n",
    "from scipy.io import wavfile\n",
    "def load_audio(path):\n",
    "    samplerate, data = wavfile.read(path)\n",
    "    return samplerate, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb18931-8abf-4553-a0af-99f5f23a4cd3",
   "metadata": {},
   "source": [
    "# Detection of illegal deforestation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a971ccc-05b8-4992-b481-d5fd93d00fd4",
   "metadata": {},
   "source": [
    "## Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66d2a8b9-60df-48e8-9684-505159e48856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /opt/anaconda3/lib/python3.11/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: soundfile in /opt/anaconda3/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.1.3)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/sarahlenet/.local/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (0.59.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/anaconda3/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "{'audio': {'path': 'pooks_6ebcaf77-aa92-4f10-984e-ecc5a919bcbb_41-44.wav', 'array': array([-0.00915527,  0.01025391, -0.01452637, ..., -0.00628662,\n",
      "        0.00064087,  0.00137329]), 'sampling_rate': 12000}, 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# installs\n",
    "!pip install librosa soundfile datasets\n",
    "\n",
    "# signing in hugging face for datasets\n",
    "from huggingface_hub import login\n",
    "token = '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'\n",
    "login(token)\n",
    "\n",
    "# train dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rfcx/frugalai\", streaming=True)\n",
    "print(next(iter(dataset['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48691020-9541-423d-bdee-7269f269703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of audio : 36000\n"
     ]
    }
   ],
   "source": [
    "# dataset size of audio\n",
    "print('length of audio : ' + str(len(next(iter(dataset['train']))['audio']['array'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d802dbc-5f2e-469b-8289-f9c772fe1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow\n",
    "import pandas\n",
    "import numpy\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b843e0-b820-4add-afb8-5b58a3718e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00915527,  0.01025391, -0.01452637, ..., -0.00628662,\n",
       "        0.00064087,  0.00137329])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of record\n",
    "next(iter(dataset['train']))['audio']['array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dea9165-dd52-4547-bc25-a31e02332277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_shards: 6\n",
       "    })\n",
       "    test: IterableDataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_shards: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset format\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee23d33-3513-4b38-9c80-bd647aba5839",
   "metadata": {},
   "source": [
    "## Spectrogram class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "592edf75-9526-45b8-9235-82da1290ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrogram saved to spectrograms/example_spectrogram.png\n"
     ]
    }
   ],
   "source": [
    "# script for transforming audio_iterable to spectrogram\n",
    "'''import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def audio_to_spectrogram(audio_iterable, save_dir=None, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    \"\"\"\n",
    "    Converts an audio file to a Mel spectrogram and saves it as an image.\n",
    "\n",
    "    Args:\n",
    "        audio_iterable (iterable): Path to the audio file.\n",
    "        save_dir (str): Directory to save the spectrogram image (optional).\n",
    "        n_fft (int): Number of FFT components.\n",
    "        hop_length (int): Hop length for the STFT.\n",
    "        n_mels (int): Number of Mel bands.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The generated Mel spectrogram (log-scaled).\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = audio_iterable['audio']['array'], audio_iterable['audio']['sampling_rate']\n",
    "    \n",
    "    # Generate the Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    \n",
    "    # Convert to log scale (dB)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Plot and save the spectrogram as an image if save_dir is specified\n",
    "    if save_dir:\n",
    "        Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "        save_path = Path(save_dir) / f\"{Path('example').stem}_spectrogram.png\" # modify the example part \n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(log_mel_spectrogram, sr=sr, hop_length=hop_length,\n",
    "                                 x_axis='time', y_axis='mel', cmap='viridis')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Mel Spectrogram')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"Spectrogram saved to {save_path}\")\n",
    "    \n",
    "    return log_mel_spectrogram\n",
    "\n",
    "# Example usage\n",
    "audio_iterable = next(iter(dataset['train'])) # Replace with your audio file path\n",
    "output_dir = \"spectrograms\"  # Replace with your desired output directory\n",
    "spectrogram = audio_to_spectrogram(audio_iterable, save_dir=output_dir)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e2eca7c-981f-438d-94df-9821ef1cb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectrogram with __iter__\n",
    "class SpectrogramIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, iterable_dataset, n_fft=2048, hop_length=512, n_mels=128, target_size=(128, 128)):\n",
    "        \"\"\"\n",
    "        Wraps an IterableDataset to preprocess audio into spectrograms.\n",
    "        \n",
    "        Args:\n",
    "            iterable_dataset (IterableDataset): The input dataset.\n",
    "            n_fft (int): Number of FFT components.\n",
    "            hop_length (int): Hop length for the STFT.\n",
    "            n_mels (int): Number of Mel bands.\n",
    "            target_size (tuple): Desired size for spectrograms (height, width).\n",
    "        \"\"\"\n",
    "        self.dataset = iterable_dataset\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def process_audio(self, audio_array, sampling_rate):\n",
    "        # Generate Mel spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(\n",
    "            y=audio_array, sr=sampling_rate, n_fft=self.n_fft, \n",
    "            hop_length=self.hop_length, n_mels=self.n_mels\n",
    "        )\n",
    "        # Convert to log scale (dB)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        log_mel_spectrogram = (log_mel_spectrogram - np.min(log_mel_spectrogram)) / (\n",
    "            np.max(log_mel_spectrogram) - np.min(log_mel_spectrogram)\n",
    "        )\n",
    "        \n",
    "        # Resize to target size\n",
    "        log_mel_spectrogram = librosa.util.fix_length(log_mel_spectrogram, size=self.target_size[1], axis=1)\n",
    "        log_mel_spectrogram = librosa.util.fix_length(log_mel_spectrogram, size=self.target_size[0], axis=0)\n",
    "        \n",
    "        return torch.tensor(log_mel_spectrogram, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sample in iter(self.dataset):  # Iterate over the base IterableDataset\n",
    "            audio_array = sample['audio']['array']\n",
    "            sampling_rate = sample['audio']['sampling_rate']\n",
    "            label = sample['label']\n",
    "            \n",
    "            # Process audio to spectrogram\n",
    "            spectrogram = self.process_audio(audio_array, sampling_rate)\n",
    "            \n",
    "            yield spectrogram, label\n",
    "    def __len__(self):\n",
    "        # Count items manually\n",
    "        return sum(1 for _ in iter(self.dataset))  # Count the number of items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9f586-f6c4-4478-b5db-7a2fd308d2b4",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "293c4449-eff8-4097-a661-677656374b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Iterate through batches\\nfor batch_idx, (spectrograms, labels) in enumerate(train_loader):\\n    print(f\"Batch {batch_idx}\")\\n    print(\"Spectrograms shape:\", spectrograms.shape)  # (batch_size, 1, height, width)\\n    print(\"Labels shape:\", labels.shape)\\n    break'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32  # Adjust based on your system's memory\n",
    "\n",
    "# Wrap the train IterableDataset\n",
    "wrapped_train_dataset = SpectrogramIterableDataset(dataset['train'])\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    wrapped_train_dataset,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, # Shuffling is not allowed for IterableDataset\n",
    "    num_workers=0 # This could be 8 as well, performance depends on available RAM. Ensure your system has enough RAM to handle multiple workers without swapping to disk.\n",
    ")\n",
    "\n",
    "'''# Iterate through batches\n",
    "for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}\")\n",
    "    print(\"Spectrograms shape:\", spectrograms.shape)  # (batch_size, 1, height, width)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d9047-aa6c-4023-b3bd-acb846adbebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Other possibility without creating the spectrogram class, \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader for the train set\n",
    "batch_size = 32  # Adjust as needed\n",
    "train_loader = DataLoader(\n",
    "    dataset['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,  # Shuffling is not allowed for IterableDataset\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Iterate through batches\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    audio_arrays = batch['audio']['array']  # Access audio data\n",
    "    labels = batch['label']  # Access labels\n",
    "    print(f\"Batch {batch_idx}\")\n",
    "    print(\"Audio arrays shape:\", audio_arrays.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ff44a-9f65-4e75-9e22-9b74119a30e6",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00334433-f198-4479-ae72-3f0a68b93a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output 1 value for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU activation and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten the output from the convolutional layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully connected layers with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Final output layer with a single neuron and sigmoid activation\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply sigmoid to output (to get a probability between 0 and 1)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Example of using BCEWithLogitsLoss (handles the sigmoid internally)\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = CNNModel()\n",
    "criterion = nn.BCEWithLogitsLoss()  # For binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ecd9c5-20dd-4a16-b2e5-936519361899",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a138b0e6-9c1a-48e6-b70a-78b2e033f361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# See whether cuda is available for GPU\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available()) # False means not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daa6da04-eef6-4971-904c-d87cff41dc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.3280, 0.4255, 0.4397,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1475, 0.3320, 0.3144,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3071, 0.3802, 0.3775,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.2711, 0.2894, 0.2162,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1515, 0.1842, 0.1277,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1413, 0.1480, 0.0774,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.5005, 0.5371, 0.4653,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.5819, 0.5546, 0.4063,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.6304, 0.6595, 0.5979,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.1368, 0.1523, 0.2050,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1529, 0.1337, 0.1328,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0491, 0.0251, 0.0325,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.3942, 0.4402, 0.4222,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3958, 0.3910, 0.3807,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3908, 0.4459, 0.4524,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.1735, 0.2083, 0.2043,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1290, 0.1825, 0.1698,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0631, 0.1314, 0.1448,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.2200, 0.2641, 0.2935,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2710, 0.3328, 0.2448,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3022, 0.3660, 0.3163,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0257, 0.1089, 0.1420,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0348, 0.0910, 0.1063,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0156, 0.0567, 0.0581,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.1766, 0.1997, 0.2133,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2900, 0.2677, 0.2444,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2725, 0.2692, 0.2796,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.3047, 0.2459, 0.2181,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2903, 0.2171, 0.1683,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2645, 0.1922, 0.1416,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.3895, 0.4520, 0.4213,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3306, 0.3792, 0.4414,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.4059, 0.4209, 0.4763,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.1094, 0.1575, 0.1898,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0727, 0.1057, 0.1480,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0025, 0.0567, 0.0483,  ..., 0.0000, 0.0000, 0.0000]]]]),\n",
       " tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c82dc5ef-8070-463e-b86e-3a8d421d7613",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'IterableDataset' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Log every 10 batches\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Step [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:475\u001b[0m, in \u001b[0;36mDataLoader.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;66;03m# NOTE [ IterableDataset and __len__ ]\u001b[39;00m\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \n\u001b[1;32m    474\u001b[0m         \u001b[38;5;66;03m# Cannot statically verify that dataset is Sized\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m         length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment, arg-type]\u001b[39;00m\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# IterableDataset doesn't allow custom sampler or batch_sampler\u001b[39;00m\n\u001b[1;32m    477\u001b[0m             \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ceil\n",
      "Cell \u001b[0;32mIn[33], line 52\u001b[0m, in \u001b[0;36mSpectrogramIterableDataset.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# If the underlying dataset has a length, return it\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'IterableDataset' has no len()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = CNNModel()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
    "        '''# Move data to GPU if available\n",
    "        spectrograms, labels = spectrograms.to('cuda'), labels.to('cuda')\n",
    "        model = model.to('cuda')'''\n",
    "        \n",
    "        labels = labels.unsqueeze(1).float()  # This reshapes labels to (batch_size, 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(spectrograms)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log loss\n",
    "        running_loss += loss.item()\n",
    "        if (batch_idx + 1) % 10 == 0:  # Log every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4439e189-ef61-466f-abbf-21f30e1720f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_shards: 6\n",
       "    })\n",
       "    test: IterableDataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_shards: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef0e7d9-cd73-4ec5-b795-af0c8aae8175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
