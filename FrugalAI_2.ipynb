{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844d5021-a372-4adf-b810-047a1d00624b",
   "metadata": {},
   "source": [
    "### FrugalAI\n",
    "#### Page\n",
    "<ul>\n",
    "<li>https://frugalaichallenge.org/tasks/</li>\n",
    "<li>https://huggingface.co/collections/frugal-ai-challenge/frugal-ai-challenge-tasks-673dd5ee724c6659a5b42443</li>\n",
    "<li>https://www.notion.so/Tips-journal-de-bord-1826269aa8b38066ae20fd7418db8dfc</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c40c848-37e2-448b-b520-a70a7cefc21d",
   "metadata": {},
   "source": [
    "### Functions that could be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06dd89c7-05a6-4a9a-85cf-cbb79695865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load audio from path\n",
    "# Import the 'wavfile' module from scipy.io to read WAV files\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# Define a function to load audio from a given file path\n",
    "def load_audio(path):\n",
    "    # 'wavfile.read' reads a WAV file and returns two values:\n",
    "    # - samplerate: the sample rate of the audio (number of samples per second)\n",
    "    # - data: the actual audio data (a numpy array with the audio samples)\n",
    "    samplerate, data = wavfile.read(path)\n",
    "    \n",
    "    # Return both the sample rate and the audio data\n",
    "    return samplerate, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94a708-12b7-4e0a-bc20-8d68bc012df3",
   "metadata": {},
   "source": [
    "### Questions\n",
    "<ul>\n",
    "<li>Do you need to shuffle records for when using only 1000, all ?</li>\n",
    "<li>Do you need to look into the creation of the spectrogram class ? (len method)</li>\n",
    "<li>Can you do that differently (with path and map function, without creating the spectrogram class ?)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2550f1-a52c-4949-a1d6-b9cd367b7a76",
   "metadata": {},
   "source": [
    "## Advice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811c432-4c9f-45df-9849-7e4bbe596401",
   "metadata": {},
   "source": [
    "<li>Eviter les conversions répétées : Si possible, stockez les données audio dans un format déjà optimisé pour PyTorch (tensors) ou un format compressé léger.</li>\n",
    "<li>Préchargement des données : Si votre dataset est statique, utilisez un système de cache (comme lru_cache ou une autre technique) pour éviter de lire les mêmes données plusieurs fois.</li>\n",
    "<li>Cible de taille minimale : Réduisez target_size à la résolution minimale nécessaire pour votre modèle. Une résolution plus petite réduit les calculs.</li>\n",
    "<li>Remplacez ReLU par LeakyReLU, qui gère mieux le problème des neurones morts.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb18931-8abf-4553-a0af-99f5f23a4cd3",
   "metadata": {},
   "source": [
    "# Detection of illegal deforestation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a971ccc-05b8-4992-b481-d5fd93d00fd4",
   "metadata": {},
   "source": [
    "## Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66d2a8b9-60df-48e8-9684-505159e48856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in ./venv/lib/python3.11/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: soundfile in ./venv/lib/python3.11/site-packages (0.13.1)\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./venv/lib/python3.11/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in ./venv/lib/python3.11/site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in ./venv/lib/python3.11/site-packages (from librosa) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in ./venv/lib/python3.11/site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=0.14 in ./venv/lib/python3.11/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./venv/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./venv/lib/python3.11/site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: pooch>=1.1 in ./venv/lib/python3.11/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./venv/lib/python3.11/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in ./venv/lib/python3.11/site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in ./venv/lib/python3.11/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./venv/lib/python3.11/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./venv/lib/python3.11/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.11/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./venv/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.11/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./venv/lib/python3.11/site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./venv/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./venv/lib/python3.11/site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "{'audio': {'path': 'pooks_6ebcaf77-aa92-4f10-984e-ecc5a919bcbb_41-44.wav', 'array': array([-0.00915527,  0.01025391, -0.01452637, ..., -0.00628662,\n",
      "        0.00064087,  0.00137329]), 'sampling_rate': 12000}, 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# installs\n",
    "!pip install librosa soundfile datasets\n",
    "\n",
    "# signing in hugging face for datasets\n",
    "from huggingface_hub import login\n",
    "token = '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'\n",
    "login(token)\n",
    "\n",
    "# train dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rfcx/frugalai\", streaming=True)\n",
    "print(next(iter(dataset['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48691020-9541-423d-bdee-7269f269703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of audio : 36000\n"
     ]
    }
   ],
   "source": [
    "# dataset size of audio\n",
    "print('length of audio : ' + str(len(next(iter(dataset['train']))['audio']['array'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d802dbc-5f2e-469b-8289-f9c772fe1ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImports explanation:\\n- `torch`: The core PyTorch library for creating tensors, defining models, and performing computations.\\n- `torch.nn`: A module containing neural network layers, such as convolutional, linear, and dropout layers.\\n- `torch.nn.functional`: Provides functions for operations like activation functions, pooling, and loss functions.\\n- `torch.optim`: Contains optimization algorithms like SGD and Adam for training neural networks.\\n- `torch.utils.data.DataLoader`: A utility to load data from a dataset and manage batching, shuffling, and parallel loading.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow\n",
    "import torchaudio\n",
    "import pandas\n",
    "import numpy\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\"\"\"\n",
    "Imports explanation:\n",
    "- `torch`: The core PyTorch library for creating tensors, defining models, and performing computations.\n",
    "- `torch.nn`: A module containing neural network layers, such as convolutional, linear, and dropout layers.\n",
    "- `torch.nn.functional`: Provides functions for operations like activation functions, pooling, and loss functions.\n",
    "- `torch.optim`: Contains optimization algorithms like SGD and Adam for training neural networks.\n",
    "- `torch.utils.data.DataLoader`: A utility to load data from a dataset and manage batching, shuffling, and parallel loading.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc3b259-5f5f-4955-97ab-15ec90b025a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20b843e0-b820-4add-afb8-5b58a3718e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00915527,  0.01025391, -0.01452637, ..., -0.00628662,\n",
       "        0.00064087,  0.00137329])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of record\n",
    "next(iter(dataset['train']))['audio']['array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dea9165-dd52-4547-bc25-a31e02332277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_shards: 6\n",
       "    })\n",
       "    test: IterableDataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_shards: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset format\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee23d33-3513-4b38-9c80-bd647aba5839",
   "metadata": {},
   "source": [
    "## Spectrogram class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc0087-0cbf-4e24-8e96-5ca5a2f15ae9",
   "metadata": {},
   "source": [
    "#### Normal script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "592edf75-9526-45b8-9235-82da1290ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrogram saved to spectrograms/example_spectrogram.png\n"
     ]
    }
   ],
   "source": [
    "# script for transforming audio_iterable to spectrogram\n",
    "'''import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def audio_to_spectrogram(audio_iterable, save_dir=None, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    \"\"\"\n",
    "    Converts an audio file to a Mel spectrogram and saves it as an image.\n",
    "\n",
    "    Args:\n",
    "        audio_iterable (iterable): Path to the audio file.\n",
    "        save_dir (str): Directory to save the spectrogram image (optional).\n",
    "        n_fft (int): Number of FFT components.\n",
    "        hop_length (int): Hop length for the STFT.\n",
    "        n_mels (int): Number of Mel bands.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The generated Mel spectrogram (log-scaled).\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = audio_iterable['audio']['array'], audio_iterable['audio']['sampling_rate']\n",
    "    \n",
    "    # Generate the Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    \n",
    "    # Convert to log scale (dB)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Plot and save the spectrogram as an image if save_dir is specified\n",
    "    if save_dir:\n",
    "        Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "        save_path = Path(save_dir) / f\"{Path('example').stem}_spectrogram.png\" # modify the example part \n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(log_mel_spectrogram, sr=sr, hop_length=hop_length,\n",
    "                                 x_axis='time', y_axis='mel', cmap='viridis')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Mel Spectrogram')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"Spectrogram saved to {save_path}\")\n",
    "    \n",
    "    return log_mel_spectrogram\n",
    "\n",
    "# Example usage\n",
    "audio_iterable = next(iter(dataset['train'])) # Replace with your audio file path\n",
    "output_dir = \"spectrograms\"  # Replace with your desired output directory\n",
    "spectrogram = audio_to_spectrogram(audio_iterable, save_dir=output_dir)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e2eca7c-981f-438d-94df-9821ef1cb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectrogram with __iter__\n",
    "class SpectrogramIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, iterable_dataset, n_fft=2048, hop_length=512, n_mels=128, target_size=(128, 128)):\n",
    "        \"\"\"\n",
    "        Wraps an IterableDataset to preprocess audio into spectrograms.\n",
    "        \n",
    "        Args:\n",
    "            iterable_dataset (IterableDataset): The input dataset.\n",
    "            n_fft (int): Number of FFT components.\n",
    "            hop_length (int): Hop length for the STFT.\n",
    "            n_mels (int): Number of Mel bands.\n",
    "            target_size (tuple): Desired size for spectrograms (height, width).\n",
    "        \"\"\"\n",
    "        self.dataset = iterable_dataset\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def process_audio(self, audio_array, sampling_rate):\n",
    "        # Generate Mel spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(\n",
    "            y=audio_array, sr=sampling_rate, n_fft=self.n_fft, \n",
    "            hop_length=self.hop_length, n_mels=self.n_mels\n",
    "        )\n",
    "        # Convert to log scale (dB)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        log_mel_spectrogram = (log_mel_spectrogram - np.min(log_mel_spectrogram)) / (\n",
    "            np.max(log_mel_spectrogram) - np.min(log_mel_spectrogram)\n",
    "        )\n",
    "        \n",
    "        # Resize to target size\n",
    "        log_mel_spectrogram = librosa.util.fix_length(log_mel_spectrogram, size=self.target_size[1], axis=1)\n",
    "        log_mel_spectrogram = librosa.util.fix_length(log_mel_spectrogram, size=self.target_size[0], axis=0)\n",
    "        \n",
    "        return torch.tensor(log_mel_spectrogram, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sample in iter(self.dataset):  # Iterate over the base IterableDataset\n",
    "            audio_array = sample['audio']['array']\n",
    "            sampling_rate = sample['audio']['sampling_rate']\n",
    "            label = sample['label']\n",
    "            \n",
    "            # Process audio to spectrogram\n",
    "            spectrogram = self.process_audio(audio_array, sampling_rate)\n",
    "            \n",
    "            yield spectrogram, label\n",
    "    def __len__(self):\n",
    "        # Count items manually\n",
    "        return sum(1 for _ in iter(self.dataset))  # Count the number of items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f874a-82f9-4ba1-b22f-2880b9fa31b9",
   "metadata": {},
   "source": [
    "#### Optimized script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4272f20e-b1c5-41e8-9839-87c196222c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpectrogramIterableDataset with torchaudio\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "class SpectrogramIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, iterable_dataset, n_fft=2048, hop_length=512, n_mels=128, target_size=(128, 128)):\n",
    "        \"\"\"\n",
    "        Wraps an IterableDataset to preprocess audio into spectrograms.\n",
    "        \n",
    "        Args:\n",
    "            iterable_dataset (IterableDataset): The input dataset.\n",
    "            n_fft (int): Number of FFT components.\n",
    "            hop_length (int): Hop length for the STFT.\n",
    "            n_mels (int): Number of Mel bands.\n",
    "            target_size (tuple): Desired size for spectrograms (height, width).\n",
    "        \"\"\"\n",
    "        self.dataset = iterable_dataset\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.target_size = target_size\n",
    "\n",
    "        # Pre-compute length if possible\n",
    "        try:\n",
    "            self._length = len(iterable_dataset)\n",
    "        except TypeError:\n",
    "            self._length = None\n",
    "\n",
    "    def process_audio(self, audio_array, sampling_rate):\n",
    "        \"\"\"\n",
    "        Convert audio data into a log Mel spectrogram tensor with normalization.\n",
    "        \"\"\"\n",
    "        waveform = torch.tensor(audio_array).unsqueeze(0)  # Convert to tensor\n",
    "        mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=sampling_rate, n_fft=self.n_fft, \n",
    "            hop_length=self.hop_length, n_mels=self.n_mels\n",
    "        )\n",
    "        mel_spectrogram = mel_transform(waveform)\n",
    "        log_mel_spectrogram = T.AmplitudeToDB()(mel_spectrogram)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        log_mel_spectrogram = (log_mel_spectrogram - log_mel_spectrogram.min()) / (\n",
    "            log_mel_spectrogram.max() - log_mel_spectrogram.min()\n",
    "        )\n",
    "        \n",
    "        # Resize to the target size # Can be removed as it is a costly operation\n",
    "        log_mel_spectrogram = torch.nn.functional.interpolate(\n",
    "            log_mel_spectrogram.unsqueeze(0), size=self.target_size\n",
    "        ).squeeze(0)\n",
    "    \n",
    "        return log_mel_spectrogram\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterate over the base dataset and yield processed spectrograms with labels.\n",
    "        \"\"\"\n",
    "        for sample in iter(self.dataset):\n",
    "            audio_array = sample['audio']['array']\n",
    "            sampling_rate = sample['audio']['sampling_rate']\n",
    "            label = sample['label']\n",
    "            \n",
    "            spectrogram = self.process_audio(audio_array, sampling_rate)\n",
    "            yield spectrogram, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the dataset if it can be pre-calculated; otherwise, calculate it dynamically.\n",
    "        \"\"\"\n",
    "        # Dynamically compute if not available\n",
    "        self._length = sum(1 for _ in iter(self.dataset))\n",
    "        return self._length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9f586-f6c4-4478-b5db-7a2fd308d2b4",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b32deb-42db-4a57-9f16-7984b4e76d3f",
   "metadata": {},
   "source": [
    "#### Normal script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "293c4449-eff8-4097-a661-677656374b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 17:09:00] offline tracker init\n",
      "[codecarbon INFO @ 17:09:00] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 17:09:00] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 17:09:06] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Mac OS detected: Please install Intel Power Gadget or enable PowerMetrics sudo to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 17:09:07] CPU Model on constant consumption mode: Intel(R) Core(TM) i5-8257U CPU @ 1.40GHz\n",
      "[codecarbon INFO @ 17:09:07] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 17:09:07] No GPU found.\n",
      "[codecarbon INFO @ 17:09:07] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 17:09:07]   Platform system: macOS-10.16-x86_64-i386-64bit\n",
      "[codecarbon INFO @ 17:09:07]   Python version: 3.11.7\n",
      "[codecarbon INFO @ 17:09:07]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 17:09:07]   Available RAM : 8.000 GB\n",
      "[codecarbon INFO @ 17:09:07]   CPU count: 8\n",
      "[codecarbon INFO @ 17:09:07]   CPU model: Intel(R) Core(TM) i5-8257U CPU @ 1.40GHz\n",
      "[codecarbon INFO @ 17:09:07]   GPU count: None\n",
      "[codecarbon INFO @ 17:09:07]   GPU model: None\n",
      "[codecarbon INFO @ 17:09:07] Saving emissions data to file /Users/sarahlenet/Desktop/FrugalAI/emissions.csv\n",
      "[codecarbon INFO @ 17:09:07] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 17:09:07] Energy consumed for RAM : 0.000000 kWh. RAM Power : 3.0 W\n",
      "[codecarbon INFO @ 17:09:07] Energy consumed for all CPUs : 0.000000 kWh. Total CPU Power : 7.5 W\n",
      "[codecarbon INFO @ 17:09:07] 0.000000 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 17:09:07] Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Loading\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32  # Adjust based on your system's memory\n",
    "\n",
    "from codecarbon import track_emissions\n",
    "\n",
    "@track_emissions(offline=True, country_iso_code=\"FRA\")\n",
    "def WrapTrainDataset(train_dataset):\n",
    "    # Wrap the train IterableDataset\n",
    "    wrapped_train_dataset = SpectrogramIterableDataset(train_dataset)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        wrapped_train_dataset,\n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, # Shuffling is not allowed for IterableDataset\n",
    "        num_workers=0 # This could be 8 as well, performance depends on available RAM. Ensure your system has enough RAM to handle multiple workers without swapping to disk.\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "'''# Iterate through batches\n",
    "for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}\")\n",
    "    print(\"Spectrograms shape:\", spectrograms.shape)  # (batch_size, 1, height, width)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    break'''\n",
    "\n",
    "\n",
    "train_loader = WrapTrainDataset(dataset['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50354813-1a4f-4df9-90ca-069207fe2216",
   "metadata": {},
   "source": [
    "#### Optimized script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288464ac-0c9c-4bdb-af61-fcecdd07cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install librosa soundfile datasets\n",
    "\n",
    "# Hugging Face login\n",
    "from huggingface_hub import login\n",
    "token = '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'\n",
    "login(token)\n",
    "\n",
    "# Load dataset with streaming\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "class StreamingAudioDataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"\n",
    "    A PyTorch IterableDataset for streaming and preprocessing audio data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_split):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with a specific split (e.g., 'train').\n",
    "        \n",
    "        Args:\n",
    "            dataset_split (iterable): Streaming dataset split (e.g., dataset['train']).\n",
    "        \"\"\"\n",
    "        self.dataset_split = dataset_split\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterator that streams and preprocesses audio data on-the-fly.\n",
    "        \n",
    "        Yields:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Preprocessed spectrogram and label tensor.\n",
    "        \"\"\"\n",
    "        for sample in self.dataset_split:\n",
    "            audio_path = sample['audio']['path']\n",
    "            label = sample['label']\n",
    "\n",
    "            # Load audio and convert to spectrogram\n",
    "            waveform, sr = librosa.load(audio_path, sr=None)\n",
    "            spectrogram = librosa.feature.melspectrogram(y=waveform, sr=sr)\n",
    "            spectrogram = torch.tensor(spectrogram, dtype=torch.float32)\n",
    "\n",
    "            # Return spectrogram and label\n",
    "            yield spectrogram, torch.tensor(label, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the dataset if it can be pre-calculated; otherwise, calculate it dynamically.\n",
    "        \"\"\"\n",
    "        # Dynamically compute if not available\n",
    "        self._length = sum(1 for _ in iter(self.dataset_split))\n",
    "        return self._length\n",
    "\n",
    "# Streaming the dataset\n",
    "dataset = load_dataset(\"rfcx/frugalai\", streaming=True)\n",
    "\n",
    "# Wrap the training data in the streaming IterableDataset\n",
    "train_dataset = StreamingAudioDataset(dataset['train'])\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,  # Adjust based on system resources\n",
    "    prefetch_factor=2,  # Preload 2 batches per worker\n",
    "    persistent_workers=True  # Keep workers alive for multi-epoch training\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d9047-aa6c-4023-b3bd-acb846adbebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Other possibility without creating the spectrogram class, \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader for the train set\n",
    "batch_size = 32  # Adjust as needed\n",
    "train_loader = DataLoader(\n",
    "    dataset['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,  # Shuffling is not allowed for IterableDataset\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Iterate through batches\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    audio_arrays = batch['audio']['array']  # Access audio data\n",
    "    labels = batch['label']  # Access labels\n",
    "    print(f\"Batch {batch_idx}\")\n",
    "    print(\"Audio arrays shape:\", audio_arrays.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ff44a-9f65-4e75-9e22-9b74119a30e6",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94223f86-d25d-42da-bd32-6513893551d0",
   "metadata": {},
   "source": [
    "#### Normal Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e95803-e94c-44a0-82d2-1447b1dcd5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CNNModel is a Convolutional Neural Network (CNN) for binary classification.\n",
    "\n",
    "    The model consists of two convolutional layers followed by a set of fully connected layers.\n",
    "    It uses ReLU activation after each convolutional and fully connected layer.\n",
    "    The final output layer produces a single value representing the probability of the positive class,\n",
    "    which is passed through a sigmoid activation function.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (nn.Conv2d): First convolutional layer.\n",
    "        conv2 (nn.Conv2d): Second convolutional layer.\n",
    "        pool (nn.MaxPool2d): Max pooling layer for downsampling.\n",
    "        fc1 (nn.Linear): First fully connected layer.\n",
    "        fc2 (nn.Linear): Second fully connected layer (output layer).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the CNNModel by defining the layers (convolutional and fully connected).\n",
    "        The model follows a standard architecture with convolutional layers for feature extraction\n",
    "        and fully connected layers for classification.\n",
    "\n",
    "        Args:\n",
    "            None: The model architecture is predefined.\n",
    "        \"\"\"\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the CNN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor with shape (batch_size, channels, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, 1), representing the probability of the positive class.\n",
    "        \"\"\"\n",
    "        # Apply conv1, ReLU activation, and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Apply conv2, ReLU activation, and max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Apply the first fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Apply the final fully connected layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply sigmoid to the output to get a probability between 0 and 1\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Example of using BCEWithLogitsLoss (handles the sigmoid internally)\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNModel()  # Create an instance of the CNNModel\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c3483-2c6e-45ba-a945-977ae644e154",
   "metadata": {},
   "source": [
    "#### Optimized script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00334433-f198-4479-ae72-3f0a68b93a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized and bettered CNNModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CNNModel is a Convolutional Neural Network (CNN) for binary classification.\n",
    "\n",
    "    The model consists of two convolutional layers with Batch Normalization, \n",
    "    followed by a set of fully connected layers with dropout for regularization.\n",
    "    It uses LeakyReLU activation for better gradient flow, and the final output \n",
    "    layer produces a single value representing the probability of the positive class.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (nn.Conv2d): First convolutional layer with 32 filters.\n",
    "        bn1 (nn.BatchNorm2d): Batch normalization for the first convolutional layer.\n",
    "        conv2 (nn.Conv2d): Second convolutional layer with 64 filters.\n",
    "        bn2 (nn.BatchNorm2d): Batch normalization for the second convolutional layer.\n",
    "        pool (nn.MaxPool2d): Max pooling layer for downsampling.\n",
    "        fc1 (nn.Linear): First fully connected layer with 128 neurons.\n",
    "        dropout (nn.Dropout): Dropout layer for regularization.\n",
    "        fc2 (nn.Linear): Second fully connected layer (output layer).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the CNNModel by defining the layers (convolutional, normalization,\n",
    "        pooling, and fully connected). This model includes dropout and Batch Normalization\n",
    "        to improve generalization and training stability.\n",
    "\n",
    "        Args:\n",
    "            None: The model architecture is predefined.\n",
    "        \"\"\"\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Define the first convolutional layer and BatchNorm\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Define the second convolutional layer and BatchNorm\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Define the MaxPooling layer for downsampling\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Define the fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)  # Adjust based on input size\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the CNN model.\n",
    "\n",
    "        The forward pass includes convolutional layers followed by Batch Normalization\n",
    "        and LeakyReLU activation, max pooling for downsampling, and fully connected layers\n",
    "        with dropout. The final layer applies a sigmoid activation to output probabilities.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor with shape (batch_size, channels, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, 1), representing the probability of the positive class.\n",
    "        \"\"\"\n",
    "        # Apply first convolutional layer, BatchNorm, LeakyReLU, and max pooling\n",
    "        x = self.pool(F.leaky_relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        # Apply second convolutional layer, BatchNorm, LeakyReLU, and max pooling\n",
    "        x = self.pool(F.leaky_relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply the first fully connected layer with dropout and ReLU activation\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        \n",
    "        # Apply the final fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Apply sigmoid to the output to get a probability between 0 and 1\n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ecd9c5-20dd-4a16-b2e5-936519361899",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a138b0e6-9c1a-48e6-b70a-78b2e033f361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# See whether cuda is available for GPU\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available()) # False means not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa6da04-eef6-4971-904c-d87cff41dc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.3280, 0.4255, 0.4397,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1475, 0.3320, 0.3144,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3071, 0.3802, 0.3775,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.2711, 0.2894, 0.2162,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1515, 0.1842, 0.1277,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1413, 0.1480, 0.0774,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.5005, 0.5371, 0.4653,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.5819, 0.5546, 0.4063,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.6304, 0.6595, 0.5979,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.1368, 0.1523, 0.2050,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1529, 0.1337, 0.1328,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0491, 0.0251, 0.0325,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.3942, 0.4402, 0.4222,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3958, 0.3910, 0.3807,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3908, 0.4459, 0.4524,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.1735, 0.2083, 0.2043,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1290, 0.1825, 0.1698,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0631, 0.1314, 0.1448,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.2200, 0.2641, 0.2935,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2710, 0.3328, 0.2448,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3022, 0.3660, 0.3163,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0257, 0.1089, 0.1420,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0348, 0.0910, 0.1063,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0156, 0.0567, 0.0581,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.1766, 0.1997, 0.2133,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2900, 0.2677, 0.2444,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2725, 0.2692, 0.2796,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.3047, 0.2459, 0.2181,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2903, 0.2171, 0.1683,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2645, 0.1922, 0.1416,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.3895, 0.4520, 0.4213,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3306, 0.3792, 0.4414,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.4059, 0.4209, 0.4763,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.1094, 0.1575, 0.1898,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0727, 0.1057, 0.1480,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0025, 0.0567, 0.0483,  ..., 0.0000, 0.0000, 0.0000]]]]),\n",
       " tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13336cfc-1992-4e83-a5c7-fa70311fb528",
   "metadata": {},
   "source": [
    "#### Normal script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82dc5ef-8070-463e-b86e-3a8d421d7613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = CNNModel()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    max_batches = 100  # Set the maximum number of batches to process\n",
    "\n",
    "    for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
    "\n",
    "        if batch_idx >= max_batches:  # Stop after 1000 batches\n",
    "            break\n",
    "        '''# Move data to GPU if available\n",
    "        spectrograms, labels = spectrograms.to('cuda'), labels.to('cuda')\n",
    "        model = model.to('cuda')'''\n",
    "        \n",
    "        labels = labels.unsqueeze(1).float()  # This reshapes labels to (batch_size, 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(spectrograms)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log loss\n",
    "        running_loss += loss.item()\n",
    "        if (batch_idx + 1) % 10 == 0:  # Log every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1bb82-aaee-4b44-bf12-31d5ec36c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6h48 - 7h20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa990ea3-f49c-4ed8-a7c8-42d4cd02c805",
   "metadata": {},
   "source": [
    "#### Optimized script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2eb8a56-ca82-4e95-ad54-41429c8634d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "'HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/rfcx/frugalai/resolve/a14fd5b7a22d5c03781db9e270162d946a49a99e/data/train-00000-of-00006.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 425f3228-80c3-48dd-85e5-582e276ed1fc)')' thrown while requesting GET https://huggingface.co/datasets/rfcx/frugalai/resolve/a14fd5b7a22d5c03781db9e270162d946a49a99e/data/train-00000-of-00006.parquet\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(13451264 bytes read, 16575744 more expected)', IncompleteRead(13451264 bytes read, 16575744 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/response.py:710\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/response.py:835\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    825\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    826\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce_content_length\n\u001b[1;32m    827\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[38;5;66;03m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[1;32m    834\u001b[0m             \u001b[38;5;66;03m# Content-Length are caught.\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_bytes_read, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining)\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(13451264 bytes read, 16575744 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/response.py:936\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 936\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/response.py:907\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m<\u001b[39m amt \u001b[38;5;129;01mand\u001b[39;00m data:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;66;03m# TODO make sure to initially read enough data to get past the headers\u001b[39;00m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;66;03m# For example, the GZ file header takes 10 bytes, we don't want to read\u001b[39;00m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;66;03m# it one byte at a time\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    908\u001b[0m     decoded_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(data, decode_content, flush_decoder)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/response.py:813\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    811\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 813\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_error_catcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfp_closed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/response.py:727\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;66;03m# This includes IncompleteRead.\u001b[39;00m\n\u001b[0;32m--> 727\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection broken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# unnecessarily.\u001b[39;00m\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(13451264 bytes read, 16575744 more expected)', IncompleteRead(13451264 bytes read, 16575744 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Wrap DataLoader with tqdm for progress bar\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(max_batches, \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (spectrograms, labels) \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_batches:  \u001b[38;5;66;03m# Stop after max_batches\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:475\u001b[0m, in \u001b[0;36mDataLoader.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;66;03m# NOTE [ IterableDataset and __len__ ]\u001b[39;00m\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \n\u001b[1;32m    474\u001b[0m         \u001b[38;5;66;03m# Cannot statically verify that dataset is Sized\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m         length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment, arg-type]\u001b[39;00m\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# IterableDataset doesn't allow custom sampler or batch_sampler\u001b[39;00m\n\u001b[1;32m    477\u001b[0m             \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ceil\n",
      "Cell \u001b[0;32mIn[25], line 71\u001b[0m, in \u001b[0;36mSpectrogramIterableDataset.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03mReturn the length of the dataset if it can be pre-calculated; otherwise, calculate it dynamically.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Dynamically compute if not available\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_length\n",
      "Cell \u001b[0;32mIn[25], line 71\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03mReturn the length of the dataset if it can be pre-calculated; otherwise, calculate it dynamically.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Dynamically compute if not available\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_length\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/iterable_dataset.py:2093\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2090\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m formatter\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   2091\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 2093\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_typed\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;49;00m\n\u001b[1;32m   2096\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;49;00m\n\u001b[1;32m   2097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_apply_feature_types_on_example\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_token_per_repo_id\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/iterable_dataset.py:279\u001b[0m, in \u001b[0;36mArrowExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m shard_example_idx_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshard_example_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    278\u001b[0m shard_example_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 279\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_tables_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgen_kwags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshard_example_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshard_example_idx_start\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_example_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/packaged_modules/parquet/parquet.py:93\u001b[0m, in \u001b[0;36mParquet._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m     91\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mor\u001b[39;00m parquet_fragment\u001b[38;5;241m.\u001b[39mrow_groups[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecord_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparquet_fragment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilter_expr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_readahead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfragment_readahead\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrecord_batch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[39;49;00m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[39;49;00m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/_dataset.pyx:3806\u001b[0m, in \u001b[0;36m_iterator\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/_dataset.pyx:3424\u001b[0m, in \u001b[0;36mpyarrow._dataset.TaggedRecordBatchIterator.__next__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyarrow/error.pxi:89\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/utils/file_utils.py:826\u001b[0m, in \u001b[0;36m_add_retries_to_file_obj_read_method.<locals>.read_with_retries\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m retry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 826\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    829\u001b[0m         aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    830\u001b[0m         asyncio\u001b[38;5;241m.\u001b[39mTimeoutError,\n\u001b[1;32m    831\u001b[0m         requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError,\n\u001b[1;32m    832\u001b[0m         requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout,\n\u001b[1;32m    833\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:757\u001b[0m, in \u001b[0;36mHfFileSystemFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:  \u001b[38;5;66;03m# block_size=0 enables fast streaming\u001b[39;00m\n\u001b[1;32m    756\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/fsspec/spec.py:1941\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1939\u001b[0m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1941\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1943\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m read: \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39m_log_stats(),\n\u001b[1;32m   1949\u001b[0m )\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/fsspec/caching.py:234\u001b[0m, in \u001b[0;36mReadAheadCache._fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    232\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, end \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_requested_bytes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# new block replaces old\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:713\u001b[0m, in \u001b[0;36mHfFileSystemFile._fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    702\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrange\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39m_api\u001b[38;5;241m.\u001b[39m_build_hf_headers(),\n\u001b[1;32m    705\u001b[0m }\n\u001b[1;32m    706\u001b[0m url \u001b[38;5;241m=\u001b[39m hf_hub_url(\n\u001b[1;32m    707\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrepo_id,\n\u001b[1;32m    708\u001b[0m     revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolved_path\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    711\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mendpoint,\n\u001b[1;32m    712\u001b[0m )\n\u001b[0;32m--> 713\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m502\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m503\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m504\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHF_HUB_DOWNLOAD_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:307\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/models.py:822\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(13451264 bytes read, 16575744 more expected)', IncompleteRead(13451264 bytes read, 16575744 more expected))"
     ]
    }
   ],
   "source": [
    "# Optimized code\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm  # For progress visualization\n",
    "from torch.cuda.amp import GradScaler, autocast  # For mixed precision training\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNModel().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "max_batches = 100  # Maximum number of batches to process per epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Wrap DataLoader with tqdm for progress bar\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=min(max_batches, len(train_loader)), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, (spectrograms, labels) in progress_bar:\n",
    "        if batch_idx >= max_batches:  # Stop after max_batches\n",
    "            break\n",
    "        \n",
    "        # Move data to the same device as the model\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "        labels = labels.unsqueeze(1).float()  # Reshape labels to (batch_size, 1)\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with autocast():\n",
    "            outputs = model(spectrograms)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "    \n",
    "    # Log average loss for the epoch\n",
    "    avg_loss = running_loss / max_batches\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92784a62-2e2c-491f-a21e-545bb86a4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changer pour checker codecarbon sur l'ensemble du loading et training et juste sur le training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
