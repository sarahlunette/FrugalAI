{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c40c848-37e2-448b-b520-a70a7cefc21d",
   "metadata": {},
   "source": [
    "### Functions that could be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd89c7-05a6-4a9a-85cf-cbb79695865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load audio from path\n",
    "# Import the 'wavfile' module from scipy.io to read WAV files\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# Define a function to load audio from a given file path\n",
    "def load_audio(path):\n",
    "    # 'wavfile.read' reads a WAV file and returns two values:\n",
    "    # - samplerate: the sample rate of the audio (number of samples per second)\n",
    "    # - data: the actual audio data (a numpy array with the audio samples)\n",
    "    samplerate, data = wavfile.read(path)\n",
    "    \n",
    "    # Return both the sample rate and the audio data\n",
    "    return samplerate, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94a708-12b7-4e0a-bc20-8d68bc012df3",
   "metadata": {},
   "source": [
    "### Questions\n",
    "<ul>\n",
    "<li>Do you need to shuffle records for when using only 1000, all ?</li>\n",
    "<li>Do you need to look into the creation of the spectrogram class ? (len method)</li>\n",
    "<li>Can you do that differently (with path and map function, without creating the spectrogram class ?)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb18931-8abf-4553-a0af-99f5f23a4cd3",
   "metadata": {},
   "source": [
    "# Detection of illegal deforestation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a971ccc-05b8-4992-b481-d5fd93d00fd4",
   "metadata": {},
   "source": [
    "## Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66d2a8b9-60df-48e8-9684-505159e48856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /opt/anaconda3/lib/python3.11/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: soundfile in /opt/anaconda3/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.1.3)\n",
      "Requirement already satisfied: joblib>=0.14 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Users/sarahlenet/.local/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (0.59.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/anaconda3/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "{'audio': {'path': 'pooks_6ebcaf77-aa92-4f10-984e-ecc5a919bcbb_41-44.wav', 'array': array([-0.00915527,  0.01025391, -0.01452637, ..., -0.00628662,\n",
      "        0.00064087,  0.00137329]), 'sampling_rate': 12000}, 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# installs\n",
    "!pip install librosa soundfile datasets\n",
    "\n",
    "# signing in hugging face for datasets\n",
    "from huggingface_hub import login\n",
    "token = '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'\n",
    "login(token)\n",
    "\n",
    "# train dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rfcx/frugalai\", streaming=True)\n",
    "print(next(iter(dataset['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48691020-9541-423d-bdee-7269f269703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of audio : 36000\n"
     ]
    }
   ],
   "source": [
    "# dataset size of audio\n",
    "print('length of audio : ' + str(len(next(iter(dataset['train']))['audio']['array'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d802dbc-5f2e-469b-8289-f9c772fe1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow\n",
    "import pandas\n",
    "import numpy\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b843e0-b820-4add-afb8-5b58a3718e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00915527,  0.01025391, -0.01452637, ..., -0.00628662,\n",
       "        0.00064087,  0.00137329])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of record\n",
    "next(iter(dataset['train']))['audio']['array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dea9165-dd52-4547-bc25-a31e02332277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDatasetDict({\n",
       "    train: IterableDataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_shards: 6\n",
       "    })\n",
       "    test: IterableDataset({\n",
       "        features: ['audio', 'label'],\n",
       "        num_shards: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset format\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee23d33-3513-4b38-9c80-bd647aba5839",
   "metadata": {},
   "source": [
    "## Spectrogram class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "592edf75-9526-45b8-9235-82da1290ab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrogram saved to spectrograms/example_spectrogram.png\n"
     ]
    }
   ],
   "source": [
    "# script for transforming audio_iterable to spectrogram\n",
    "'''import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def audio_to_spectrogram(audio_iterable, save_dir=None, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    \"\"\"\n",
    "    Converts an audio file to a Mel spectrogram and saves it as an image.\n",
    "\n",
    "    Args:\n",
    "        audio_iterable (iterable): Path to the audio file.\n",
    "        save_dir (str): Directory to save the spectrogram image (optional).\n",
    "        n_fft (int): Number of FFT components.\n",
    "        hop_length (int): Hop length for the STFT.\n",
    "        n_mels (int): Number of Mel bands.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The generated Mel spectrogram (log-scaled).\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = audio_iterable['audio']['array'], audio_iterable['audio']['sampling_rate']\n",
    "    \n",
    "    # Generate the Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    \n",
    "    # Convert to log scale (dB)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Plot and save the spectrogram as an image if save_dir is specified\n",
    "    if save_dir:\n",
    "        Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "        save_path = Path(save_dir) / f\"{Path('example').stem}_spectrogram.png\" # modify the example part \n",
    "        \n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(log_mel_spectrogram, sr=sr, hop_length=hop_length,\n",
    "                                 x_axis='time', y_axis='mel', cmap='viridis')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Mel Spectrogram')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        print(f\"Spectrogram saved to {save_path}\")\n",
    "    \n",
    "    return log_mel_spectrogram\n",
    "\n",
    "# Example usage\n",
    "audio_iterable = next(iter(dataset['train'])) # Replace with your audio file path\n",
    "output_dir = \"spectrograms\"  # Replace with your desired output directory\n",
    "spectrogram = audio_to_spectrogram(audio_iterable, save_dir=output_dir)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e2eca7c-981f-438d-94df-9821ef1cb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectrogram with __iter__\n",
    "class SpectrogramIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, iterable_dataset, n_fft=2048, hop_length=512, n_mels=128, target_size=(128, 128)):\n",
    "        \"\"\"\n",
    "        Wraps an IterableDataset to preprocess audio into spectrograms.\n",
    "        \n",
    "        Args:\n",
    "            iterable_dataset (IterableDataset): The input dataset.\n",
    "            n_fft (int): Number of FFT components.\n",
    "            hop_length (int): Hop length for the STFT.\n",
    "            n_mels (int): Number of Mel bands.\n",
    "            target_size (tuple): Desired size for spectrograms (height, width).\n",
    "        \"\"\"\n",
    "        self.dataset = iterable_dataset\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def process_audio(self, audio_array, sampling_rate):\n",
    "        # Generate Mel spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(\n",
    "            y=audio_array, sr=sampling_rate, n_fft=self.n_fft, \n",
    "            hop_length=self.hop_length, n_mels=self.n_mels\n",
    "        )\n",
    "        # Convert to log scale (dB)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        log_mel_spectrogram = (log_mel_spectrogram - np.min(log_mel_spectrogram)) / (\n",
    "            np.max(log_mel_spectrogram) - np.min(log_mel_spectrogram)\n",
    "        )\n",
    "        \n",
    "        # Resize to target size\n",
    "        log_mel_spectrogram = librosa.util.fix_length(log_mel_spectrogram, size=self.target_size[1], axis=1)\n",
    "        log_mel_spectrogram = librosa.util.fix_length(log_mel_spectrogram, size=self.target_size[0], axis=0)\n",
    "        \n",
    "        return torch.tensor(log_mel_spectrogram, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sample in iter(self.dataset):  # Iterate over the base IterableDataset\n",
    "            audio_array = sample['audio']['array']\n",
    "            sampling_rate = sample['audio']['sampling_rate']\n",
    "            label = sample['label']\n",
    "            \n",
    "            # Process audio to spectrogram\n",
    "            spectrogram = self.process_audio(audio_array, sampling_rate)\n",
    "            \n",
    "            yield spectrogram, label\n",
    "    def __len__(self):\n",
    "        # Count items manually\n",
    "        return sum(1 for _ in iter(self.dataset))  # Count the number of items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c9f586-f6c4-4478-b5db-7a2fd308d2b4",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "293c4449-eff8-4097-a661-677656374b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Iterate through batches\\nfor batch_idx, (spectrograms, labels) in enumerate(train_loader):\\n    print(f\"Batch {batch_idx}\")\\n    print(\"Spectrograms shape:\", spectrograms.shape)  # (batch_size, 1, height, width)\\n    print(\"Labels shape:\", labels.shape)\\n    break'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32  # Adjust based on your system's memory\n",
    "\n",
    "# Wrap the train IterableDataset\n",
    "wrapped_train_dataset = SpectrogramIterableDataset(dataset['train'])\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    wrapped_train_dataset,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, # Shuffling is not allowed for IterableDataset\n",
    "    num_workers=0 # This could be 8 as well, performance depends on available RAM. Ensure your system has enough RAM to handle multiple workers without swapping to disk.\n",
    ")\n",
    "\n",
    "'''# Iterate through batches\n",
    "for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}\")\n",
    "    print(\"Spectrograms shape:\", spectrograms.shape)  # (batch_size, 1, height, width)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    break'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d9047-aa6c-4023-b3bd-acb846adbebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Other possibility without creating the spectrogram class, \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader for the train set\n",
    "batch_size = 32  # Adjust as needed\n",
    "train_loader = DataLoader(\n",
    "    dataset['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,  # Shuffling is not allowed for IterableDataset\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Iterate through batches\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    audio_arrays = batch['audio']['array']  # Access audio data\n",
    "    labels = batch['label']  # Access labels\n",
    "    print(f\"Batch {batch_idx}\")\n",
    "    print(\"Audio arrays shape:\", audio_arrays.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ff44a-9f65-4e75-9e22-9b74119a30e6",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00334433-f198-4479-ae72-3f0a68b93a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CNNModel is a Convolutional Neural Network (CNN) for binary classification.\n",
    "\n",
    "    The model consists of two convolutional layers followed by a set of fully connected layers.\n",
    "    It uses ReLU activation after each convolutional and fully connected layer.\n",
    "    The final output layer produces a single value representing the probability of the positive class,\n",
    "    which is passed through a sigmoid activation function.\n",
    "\n",
    "    Attributes:\n",
    "        conv1 (nn.Conv2d): First convolutional layer.\n",
    "        conv2 (nn.Conv2d): Second convolutional layer.\n",
    "        pool (nn.MaxPool2d): Max pooling layer for downsampling.\n",
    "        fc1 (nn.Linear): First fully connected layer.\n",
    "        fc2 (nn.Linear): Second fully connected layer (output layer).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the CNNModel by defining the layers (convolutional and fully connected).\n",
    "        The model follows a standard architecture with convolutional layers for feature extraction\n",
    "        and fully connected layers for classification.\n",
    "\n",
    "        Args:\n",
    "            None: The model architecture is predefined.\n",
    "        \"\"\"\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the CNN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor with shape (batch_size, channels, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (batch_size, 1), representing the probability of the positive class.\n",
    "        \"\"\"\n",
    "        # Apply conv1, ReLU activation, and max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Apply conv2, ReLU activation, and max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Apply the first fully connected layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # Apply the final fully connected layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply sigmoid to the output to get a probability between 0 and 1\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Example of using BCEWithLogitsLoss (handles the sigmoid internally)\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNModel()  # Create an instance of the CNNModel\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ecd9c5-20dd-4a16-b2e5-936519361899",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a138b0e6-9c1a-48e6-b70a-78b2e033f361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# See whether cuda is available for GPU\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available()) # False means not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daa6da04-eef6-4971-904c-d87cff41dc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.3280, 0.4255, 0.4397,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1475, 0.3320, 0.3144,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3071, 0.3802, 0.3775,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.2711, 0.2894, 0.2162,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1515, 0.1842, 0.1277,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1413, 0.1480, 0.0774,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.5005, 0.5371, 0.4653,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.5819, 0.5546, 0.4063,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.6304, 0.6595, 0.5979,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.1368, 0.1523, 0.2050,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1529, 0.1337, 0.1328,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0491, 0.0251, 0.0325,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.3942, 0.4402, 0.4222,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3958, 0.3910, 0.3807,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3908, 0.4459, 0.4524,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.1735, 0.2083, 0.2043,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.1290, 0.1825, 0.1698,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0631, 0.1314, 0.1448,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.2200, 0.2641, 0.2935,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2710, 0.3328, 0.2448,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3022, 0.3660, 0.3163,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0257, 0.1089, 0.1420,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0348, 0.0910, 0.1063,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0156, 0.0567, 0.0581,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.1766, 0.1997, 0.2133,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2900, 0.2677, 0.2444,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2725, 0.2692, 0.2796,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.3047, 0.2459, 0.2181,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2903, 0.2171, 0.1683,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.2645, 0.1922, 0.1416,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       " \n",
       " \n",
       "         [[[0.3895, 0.4520, 0.4213,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.3306, 0.3792, 0.4414,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.4059, 0.4209, 0.4763,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.1094, 0.1575, 0.1898,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0727, 0.1057, 0.1480,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0025, 0.0567, 0.0483,  ..., 0.0000, 0.0000, 0.0000]]]]),\n",
       " tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82dc5ef-8070-463e-b86e-3a8d421d7613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = CNNModel()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    max_batches = 100  # Set the maximum number of batches to process\n",
    "\n",
    "    for batch_idx, (spectrograms, labels) in enumerate(train_loader):\n",
    "\n",
    "        if batch_idx >= max_batches:  # Stop after 1000 batches\n",
    "            break\n",
    "        '''# Move data to GPU if available\n",
    "        spectrograms, labels = spectrograms.to('cuda'), labels.to('cuda')\n",
    "        model = model.to('cuda')'''\n",
    "        \n",
    "        labels = labels.unsqueeze(1).float()  # This reshapes labels to (batch_size, 1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(spectrograms)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log loss\n",
    "        running_loss += loss.item()\n",
    "        if (batch_idx + 1) % 10 == 0:  # Log every 10 batches\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1bb82-aaee-4b44-bf12-31d5ec36c301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6h40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cc8b1-f2ad-405f-9cf9-ff681a0da317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
